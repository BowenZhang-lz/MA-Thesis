{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21bd63e6-46b0-4565-b14d-11898ee15736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Reddit API connection...\n",
      "✓ PRAW initialized successfully\n",
      "✓ r/ChatGPT access successful - 10,679,983 subscribers\n",
      "✓ Search functionality working - found 3 test results\n",
      "✓ Post details accessible - Title: 'No, your LLM is not sentient, not reaching conscio...'\n",
      "\n",
      "🎉 All tests passed! Reddit API is ready for scraping.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import sys\n",
    "\n",
    "def test_reddit_api():\n",
    "    \"\"\"Quick test of Reddit API credentials\"\"\"\n",
    "    print(\"Testing Reddit API connection...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Reddit instance\n",
    "        reddit = praw.Reddit(\n",
    "            client_id='PBx7xhwyKwGPIBf9-4cBqw',        \n",
    "            client_secret='Ap2G7IVid1DpA8TpO6lb0y0UQhx4-g',  \n",
    "            user_agent='chuckyeager'\n",
    "        )\n",
    "        \n",
    "        # Test basic access\n",
    "        print(\"✓ PRAW initialized successfully\")\n",
    "        \n",
    "        # Test subreddit access\n",
    "        chatgpt_sub = reddit.subreddit('ChatGPT')\n",
    "        print(f\"✓ r/ChatGPT access successful - {chatgpt_sub.subscribers:,} subscribers\")\n",
    "        \n",
    "        # Test search functionality\n",
    "        search_results = list(chatgpt_sub.search('consciousness', limit=3))\n",
    "        print(f\"✓ Search functionality working - found {len(search_results)} test results\")\n",
    "        \n",
    "        # Test post details\n",
    "        if search_results:\n",
    "            test_post = search_results[0]\n",
    "            print(f\"✓ Post details accessible - Title: '{test_post.title[:50]}...'\")\n",
    "        \n",
    "        print(\"\\n🎉 All tests passed! Reddit API is ready for scraping.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Reddit API test failed: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Check your internet connection\")\n",
    "        print(\"2. Verify Reddit credentials are correct\")\n",
    "        print(\"3. Ensure Reddit app permissions are set properly\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_reddit_api()\n",
    "    sys.exit(0 if success else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea501d2-57f1-455c-927f-ffc9b1244ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 === COMPREHENSIVE AI CONSCIOUS DISCOURSE SCRAPER ===\n",
      "📅 Analyzing 3+ years of 'AI conscious' discussions on Reddit\n",
      "📁 Output directory: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new\n",
      "✅ Reddit API: Connected - r/artificial has 1,101,533 subscribers\n",
      "✅ Search function: Working - found 3 test results for 'AI conscious'\n",
      "\n",
      "📊 Scraping Parameters:\n",
      "   🔍 Keyword: 'AI conscious'\n",
      "   📅 Period: 2022-06-11 to 2025-06-16\n",
      "   🏛️  Subreddits: 37 communities\n",
      "   ⏱️  Expected Duration: 2-3 hours\n",
      "\n",
      "🚀 Starting comprehensive scraping...\n",
      "\n",
      "================================================================================\n",
      "🎯 COMPREHENSIVE AI CONSCIOUS DISCOURSE SCRAPING\n",
      "📅 Time Period: 2022-06-11 to 2025-06-16\n",
      "🔍 Keyword: 'AI conscious'\n",
      "📊 Target: ~3 years of AI consciousness discussions\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching Reddit for: 'AI conscious'\n",
      "📊 Scanning 37 subreddits...\n",
      "\n",
      "[1/37] Scanning r/artificial...\n",
      "    search_new: 167 posts\n",
      "    search_relevance: 3 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/q46wkhss5rd_g6bkyzhn33q00000gn/T/ipykernel_93303/2302395670.py:95: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  ('top_year_filtered', lambda: [p for p in subreddit.top('year', limit=200)\n",
      "/var/folders/d1/q46wkhss5rd_g6bkyzhn33q00000gn/T/ipykernel_93303/2302395670.py:97: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  ('top_all_filtered', lambda: [p for p in subreddit.top('all', limit=200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📊 r/artificial total: 170 posts\n",
      "\n",
      "[2/37] Scanning r/ArtificialIntelligence...\n",
      "    Error with search_new: received 404 HTTP response\n",
      "    Error with search_relevance: received 404 HTTP response\n",
      "    Error with search_top: received 404 HTTP response\n",
      "    Error with hot_filtered: received 404 HTTP response\n",
      "    Error with top_year_filtered: received 404 HTTP response\n",
      "    Error with top_all_filtered: received 404 HTTP response\n",
      "  📊 r/ArtificialIntelligence total: 0 posts\n",
      "\n",
      "[3/37] Scanning r/MachineLearning...\n",
      "    search_relevance: 22 posts\n",
      "  📊 r/MachineLearning total: 22 posts\n",
      "\n",
      "[4/37] Scanning r/deeplearning...\n",
      "    search_new: 17 posts\n",
      "  📊 r/deeplearning total: 17 posts\n",
      "\n",
      "[5/37] Scanning r/ChatGPT...\n",
      "      Error processing submission 1kpkws5: received 429 HTTP response\n",
      "      Error processing submission 1kpi8np: received 429 HTTP response\n",
      "      Error processing submission 1kpgucv: received 429 HTTP response\n",
      "      Error processing submission 1kpe7p1: received 429 HTTP response\n",
      "      Error processing submission 1kpcr8u: received 429 HTTP response\n",
      "      Error processing submission 1kp8lyz: received 429 HTTP response\n",
      "    search_new: 238 posts\n",
      "    search_relevance: 165 posts\n",
      "    search_top: 162 posts\n",
      "  📊 r/ChatGPT total: 565 posts\n",
      "\n",
      "[6/37] Scanning r/OpenAI...\n",
      "    search_new: 135 posts\n",
      "    search_relevance: 6 posts\n",
      "  📊 r/OpenAI total: 141 posts\n",
      "\n",
      "[7/37] Scanning r/singularity...\n",
      "      Error processing submission 1jwrvoc: received 429 HTTP response\n",
      "      Error processing submission 1jvefzy: received 429 HTTP response\n",
      "      Error processing submission 1jumi16: received 429 HTTP response\n",
      "      Error processing submission 1jp4c40: received 429 HTTP response\n",
      "    search_new: 217 posts\n",
      "    search_relevance: 63 posts\n",
      "    search_top: 22 posts\n",
      "  📊 r/singularity total: 302 posts\n",
      "\n",
      "[8/37] Scanning r/LocalLLaMA...\n",
      "    search_new: 35 posts\n",
      "    search_relevance: 5 posts\n",
      "  📊 r/LocalLLaMA total: 40 posts\n",
      "\n",
      "[9/37] Scanning r/Anthropic...\n",
      "    search_new: 12 posts\n",
      "    search_relevance: 1 posts\n",
      "  📊 r/Anthropic total: 13 posts\n",
      "\n",
      "[10/37] Scanning r/GoogleAI...\n",
      "  📊 r/GoogleAI total: 0 posts\n",
      "\n",
      "[11/37] Scanning r/philosophy...\n",
      "    search_new: 16 posts\n",
      "    search_relevance: 1 posts\n",
      "  📊 r/philosophy total: 17 posts\n",
      "\n",
      "[12/37] Scanning r/consciousness...\n",
      "      Error processing submission 1esem87: received 429 HTTP response\n",
      "      Error processing submission 1epp1rz: received 429 HTTP response\n",
      "      Error processing submission 1eow6qe: received 429 HTTP response\n",
      "      Error processing submission 1envdab: received 429 HTTP response\n",
      "      Error processing submission 1endfnu: received 429 HTTP response\n",
      "      Error processing submission 1enc7bb: received 429 HTTP response\n",
      "      Error processing submission 1elr48i: received 429 HTTP response\n",
      "      Error processing submission 1eiibl8: received 429 HTTP response\n",
      "    search_new: 219 posts\n",
      "    search_relevance: 12 posts\n",
      "  📊 r/consciousness total: 231 posts\n",
      "\n",
      "[13/37] Scanning r/PhilosophyOfMind...\n",
      "    search_new: 6 posts\n",
      "  📊 r/PhilosophyOfMind total: 6 posts\n",
      "\n",
      "[14/37] Scanning r/AskPhilosophy...\n",
      "    search_new: 90 posts\n",
      "    search_relevance: 5 posts\n",
      "  📊 r/AskPhilosophy total: 95 posts\n",
      "\n",
      "[15/37] Scanning r/neuroscience...\n",
      "  📊 r/neuroscience total: 0 posts\n",
      "\n",
      "[16/37] Scanning r/cogsci...\n",
      "    search_new: 16 posts\n",
      "  📊 r/cogsci total: 16 posts\n",
      "\n",
      "[17/37] Scanning r/technology...\n",
      "    search_new: 16 posts\n",
      "    search_relevance: 2 posts\n",
      "  📊 r/technology total: 18 posts\n",
      "\n",
      "[18/37] Scanning r/tech...\n",
      "  📊 r/tech total: 0 posts\n",
      "\n",
      "[19/37] Scanning r/Futurology...\n",
      "    search_new: 72 posts\n",
      "    search_relevance: 1 posts\n",
      "  📊 r/Futurology total: 73 posts\n",
      "\n",
      "[20/37] Scanning r/ComputerScience...\n",
      "    search_new: 1 posts\n",
      "    search_relevance: 2 posts\n",
      "  📊 r/ComputerScience total: 3 posts\n",
      "\n",
      "[21/37] Scanning r/programming...\n",
      "    search_new: 2 posts\n",
      "  📊 r/programming total: 2 posts\n",
      "\n",
      "[22/37] Scanning r/MachineLearning...\n",
      "  📊 r/MachineLearning total: 0 posts\n",
      "\n",
      "[23/37] Scanning r/datascience...\n",
      "    search_new: 3 posts\n",
      "  📊 r/datascience total: 3 posts\n",
      "\n",
      "[24/37] Scanning r/LessWrong...\n",
      "    search_new: 4 posts\n",
      "  📊 r/LessWrong total: 4 posts\n",
      "\n",
      "[25/37] Scanning r/slatestarcodex...\n",
      "    search_new: 50 posts\n",
      "  📊 r/slatestarcodex total: 50 posts\n",
      "\n",
      "[26/37] Scanning r/ControlProblem...\n",
      "    search_new: 32 posts\n",
      "    search_relevance: 2 posts\n",
      "  📊 r/ControlProblem total: 34 posts\n",
      "\n",
      "[27/37] Scanning r/AISafety...\n",
      "    search_new: 5 posts\n",
      "  📊 r/AISafety total: 5 posts\n",
      "\n",
      "[28/37] Scanning r/rational...\n",
      "    search_new: 3 posts\n",
      "  📊 r/rational total: 3 posts\n",
      "\n",
      "[29/37] Scanning r/todayilearned...\n",
      "  📊 r/todayilearned total: 0 posts\n",
      "\n",
      "[30/37] Scanning r/explainlikeimfive...\n",
      "  📊 r/explainlikeimfive total: 0 posts\n",
      "\n",
      "[31/37] Scanning r/askreddit...\n",
      "    search_new: 52 posts\n",
      "    search_relevance: 20 posts\n",
      "  📊 r/askreddit total: 72 posts\n",
      "\n",
      "[32/37] Scanning r/news...\n",
      "  📊 r/news total: 0 posts\n",
      "\n",
      "[33/37] Scanning r/worldnews...\n",
      "  📊 r/worldnews total: 0 posts\n",
      "\n",
      "[34/37] Scanning r/science...\n",
      "    search_new: 1 posts\n",
      "    search_relevance: 1 posts\n",
      "  📊 r/science total: 2 posts\n",
      "\n",
      "[35/37] Scanning r/Showerthoughts...\n",
      "    search_new: 10 posts\n",
      "    search_relevance: 3 posts\n",
      "  📊 r/Showerthoughts total: 13 posts\n",
      "\n",
      "[36/37] Scanning r/unpopularopinion...\n",
      "    search_relevance: 1 posts\n",
      "  📊 r/unpopularopinion total: 1 posts\n",
      "\n",
      "[37/37] Scanning r/changemyview...\n",
      "    search_new: 32 posts\n",
      "    search_relevance: 1 posts\n",
      "  📊 r/changemyview total: 33 posts\n",
      "\n",
      "✅ Comprehensive scraping completed!\n",
      "📊 Total posts collected: 1,951\n",
      "📅 Time span: 2022-06-11 to 2025-06-16\n",
      "\n",
      "💾 Saving comprehensive dataset...\n",
      "✅ Main dataset saved to: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new/reddit_ai_conscious_comprehensive_2022_2025.csv\n",
      "📊 Total records: 1,951\n",
      "📊 Analysis dataset saved to: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new/reddit_ai_conscious_for_analysis.csv\n",
      "🔍 Sample dataset saved to: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new/reddit_ai_conscious_sample.csv\n",
      "\n",
      "📊 Generating comprehensive statistics...\n",
      "📊 Statistics saved to: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new/reddit_ai_conscious_comprehensive_statistics.csv\n",
      "📅 Temporal breakdown saved to: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new/reddit_ai_conscious_temporal_breakdown.csv\n",
      "📊 Subreddit breakdown saved to: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new/reddit_ai_conscious_subreddit_breakdown.csv\n",
      "\n",
      "📊 === COMPREHENSIVE AI CONSCIOUS DISCOURSE ANALYSIS ===\n",
      "📅 Period: 2022-06-11T06:29:49 to 2025-06-15T22:42:52\n",
      "📊 Total Posts: 1,951\n",
      "📅 Time Span: 1101 days\n",
      "🏛️  Coverage: 28 subreddits\n",
      "👥 Authors: 1,471\n",
      "🏆 High Quality Posts: 262\n",
      "🧠 High Consciousness Relevance: 1,367\n",
      "📈 Average Score: 69.4\n",
      "💬 Average Comments: 44.7\n",
      "📅 Peak Discussion Month: 2025-05\n",
      "\n",
      "📊 Posts by Year:\n",
      "   2022: 108 posts\n",
      "   2023: 565 posts\n",
      "   2024: 531 posts\n",
      "   2025: 747 posts\n",
      "\n",
      "🏛️  Top Subreddits:\n",
      "   r/ChatGPT: 565 posts\n",
      "   r/singularity: 302 posts\n",
      "   r/consciousness: 231 posts\n",
      "   r/artificial: 170 posts\n",
      "   r/OpenAI: 141 posts\n",
      "\n",
      "🎉 === COMPREHENSIVE SCRAPING COMPLETE ===\n",
      "📊 Total 'AI conscious' posts collected: 1,951\n",
      "📁 All files saved in: /Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new\n",
      "📅 Dataset spans: 2022-06-11 to 2025-06-16\n",
      "\n",
      "📋 Generated Files:\n",
      "   • reddit_ai_conscious_comprehensive_2022_2025.csv (main dataset)\n",
      "   • reddit_ai_conscious_for_analysis.csv (analysis-ready)\n",
      "   • reddit_ai_conscious_sample.csv (preview)\n",
      "   • reddit_ai_conscious_comprehensive_statistics.csv\n",
      "   • reddit_ai_conscious_temporal_breakdown.csv\n",
      "   • reddit_ai_conscious_subreddit_breakdown.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ComprehensiveAIConsciousScraper:\n",
    "    def __init__(self):\n",
    "        # Reddit API credentials\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id='PBx7xhwyKwGPIBf9-4cBqw',        \n",
    "            client_secret='Ap2G7IVid1DpA8TpO6lb0y0UQhx4-g',  \n",
    "            user_agent='chuckyeager'\n",
    "        )\n",
    "        \n",
    "        # Single keyword focus\n",
    "        self.keyword = \"AI conscious\"\n",
    "        \n",
    "        # Comprehensive time period: LaMDA controversy to present\n",
    "        self.start_date = \"2022-06-11\"  # Google LaMDA sentience controversy start\n",
    "        self.end_date = \"2025-06-16\"    # Today\n",
    "        \n",
    "        # Output directory\n",
    "        self.output_dir = \"/Users/zhangbowen/Downloads/MA Thesis BWZ/code/reddit new\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Comprehensive subreddit list for AI consciousness discussions\n",
    "        self.subreddits = [\n",
    "            # AI-specific subreddits\n",
    "            \"artificial\", \"ArtificialIntelligence\", \"MachineLearning\", \n",
    "            \"deeplearning\", \"ChatGPT\", \"OpenAI\", \"singularity\",\n",
    "            \"LocalLLaMA\", \"Anthropic\", \"GoogleAI\",\n",
    "            \n",
    "            # Philosophy and consciousness subreddits\n",
    "            \"philosophy\", \"consciousness\", \"PhilosophyOfMind\", \n",
    "            \"AskPhilosophy\", \"neuroscience\", \"cogsci\",\n",
    "            \n",
    "            # Technology and futurism\n",
    "            \"technology\", \"tech\", \"Futurology\", \"ComputerScience\",\n",
    "            \"programming\", \"MachineLearning\", \"datascience\",\n",
    "            \n",
    "            # Rationalist and AI safety communities\n",
    "            \"LessWrong\", \"slatestarcodex\", \"ControlProblem\",\n",
    "            \"AISafety\", \"rational\",\n",
    "            \n",
    "            # General discussion communities\n",
    "            \"todayilearned\", \"explainlikeimfive\", \"askreddit\",\n",
    "            \"news\", \"worldnews\", \"science\", \"Showerthoughts\",\n",
    "            \"unpopularopinion\", \"changemyview\"\n",
    "        ]\n",
    "        \n",
    "    def get_date_range_unix(self, start_date, end_date):\n",
    "        \"\"\"Convert date strings to unix timestamps\"\"\"\n",
    "        start_unix = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "        end_unix = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "        return start_unix, end_unix\n",
    "    \n",
    "    def rate_limit(self):\n",
    "        \"\"\"Implement respectful rate limiting\"\"\"\n",
    "        delay = random.uniform(0.5, 2)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def scrape_ai_conscious_discussions(self):\n",
    "        \"\"\"Scrape all Reddit discussions mentioning 'AI conscious' from 2022-06-11 to today\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🎯 COMPREHENSIVE AI CONSCIOUS DISCOURSE SCRAPING\")\n",
    "        print(f\"📅 Time Period: {self.start_date} to {self.end_date}\")\n",
    "        print(f\"🔍 Keyword: '{self.keyword}'\")\n",
    "        print(f\"📊 Target: ~3 years of AI consciousness discussions\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        results = []\n",
    "        start_unix, end_unix = self.get_date_range_unix(self.start_date, self.end_date)\n",
    "        \n",
    "        print(f\"\\n🔍 Searching Reddit for: '{self.keyword}'\")\n",
    "        print(f\"📊 Scanning {len(self.subreddits)} subreddits...\")\n",
    "        \n",
    "        total_posts_found = 0\n",
    "        \n",
    "        for i, subreddit_name in enumerate(self.subreddits, 1):\n",
    "            print(f\"\\n[{i}/{len(self.subreddits)}] Scanning r/{subreddit_name}...\")\n",
    "            \n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                \n",
    "                # Multiple search strategies for comprehensive coverage\n",
    "                search_strategies = [\n",
    "                    ('search_new', lambda: subreddit.search(self.keyword, sort='new', time_filter='all', limit=500)),\n",
    "                    ('search_relevance', lambda: subreddit.search(self.keyword, sort='relevance', time_filter='all', limit=300)),\n",
    "                    ('search_top', lambda: subreddit.search(self.keyword, sort='top', time_filter='all', limit=300)),\n",
    "                    ('hot_filtered', lambda: [p for p in subreddit.hot(limit=200) \n",
    "                                            if self.keyword.lower() in (p.title + \" \" + p.selftext).lower()]),\n",
    "                    ('top_year_filtered', lambda: [p for p in subreddit.top('year', limit=200) \n",
    "                                                  if self.keyword.lower() in (p.title + \" \" + p.selftext).lower()]),\n",
    "                    ('top_all_filtered', lambda: [p for p in subreddit.top('all', limit=200) \n",
    "                                                 if self.keyword.lower() in (p.title + \" \" + p.selftext).lower()])\n",
    "                ]\n",
    "                \n",
    "                subreddit_posts = 0\n",
    "                \n",
    "                for strategy_name, search_func in search_strategies:\n",
    "                    try:\n",
    "                        search_results = search_func()\n",
    "                        strategy_posts = 0\n",
    "                        \n",
    "                        for submission in search_results:\n",
    "                            # Check if post is within our comprehensive time range\n",
    "                            if start_unix <= submission.created_utc <= end_unix:\n",
    "                                try:\n",
    "                                    # Check for duplicate posts\n",
    "                                    if any(r['post_id'] == submission.id for r in results):\n",
    "                                        continue\n",
    "                                    \n",
    "                                    # Get comments for discourse analysis\n",
    "                                    submission.comments.replace_more(limit=0)\n",
    "                                    \n",
    "                                    # Extract comprehensive comment data\n",
    "                                    top_comments = []\n",
    "                                    comment_count = 0\n",
    "                                    for comment in submission.comments:\n",
    "                                        if hasattr(comment, 'body') and len(comment.body) > 30 and comment_count < 10:\n",
    "                                            top_comments.append({\n",
    "                                                'body': comment.body[:500],  # Extended for analysis\n",
    "                                                'score': comment.score,\n",
    "                                                'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                                                'created_utc': comment.created_utc,\n",
    "                                                'created_date': datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                                                'depth': getattr(comment, 'depth', 0)\n",
    "                                            })\n",
    "                                            comment_count += 1\n",
    "                                    \n",
    "                                    # Create comprehensive record\n",
    "                                    result = {\n",
    "                                        # Basic metadata\n",
    "                                        'platform': 'reddit',\n",
    "                                        'keyword': self.keyword,\n",
    "                                        'search_strategy': strategy_name,\n",
    "                                        'subreddit': subreddit_name,\n",
    "                                        'post_id': submission.id,\n",
    "                                        'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                                        \n",
    "                                        # Content\n",
    "                                        'title': submission.title,\n",
    "                                        'content': submission.selftext,\n",
    "                                        'url': submission.url,\n",
    "                                        'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                                        \n",
    "                                        # Timestamps\n",
    "                                        'created_utc': submission.created_utc,\n",
    "                                        'created_date': datetime.fromtimestamp(submission.created_utc).isoformat(),\n",
    "                                        'created_year': datetime.fromtimestamp(submission.created_utc).year,\n",
    "                                        'created_month': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m'),\n",
    "                                        'days_since_lamda': (submission.created_utc - \n",
    "                                                           datetime.strptime(self.start_date, \"%Y-%m-%d\").timestamp()) / 86400,\n",
    "                                        \n",
    "                                        # Engagement metrics\n",
    "                                        'score': submission.score,\n",
    "                                        'upvote_ratio': submission.upvote_ratio,\n",
    "                                        'num_comments': submission.num_comments,\n",
    "                                        'gilded': submission.gilded,\n",
    "                                        'is_self_post': submission.is_self,\n",
    "                                        'is_stickied': submission.stickied,\n",
    "                                        'is_over_18': submission.over_18,\n",
    "                                        \n",
    "                                        # Analysis metrics\n",
    "                                        'discussion_quality': self.assess_discussion_quality(submission),\n",
    "                                        'consciousness_relevance': self.assess_consciousness_relevance(\n",
    "                                            submission.title + \" \" + submission.selftext\n",
    "                                        ),\n",
    "                                        'text_length': len(submission.title + \" \" + submission.selftext),\n",
    "                                        \n",
    "                                        # Comments data for discourse analysis\n",
    "                                        'top_comments': json.dumps(top_comments),\n",
    "                                        'comment_analysis': self.analyze_comments(top_comments),\n",
    "                                        \n",
    "                                        # Collection metadata\n",
    "                                        'collection_timestamp': datetime.now().isoformat(),\n",
    "                                        'collection_method': 'comprehensive_temporal_scraper'\n",
    "                                    }\n",
    "                                    \n",
    "                                    results.append(result)\n",
    "                                    strategy_posts += 1\n",
    "                                    subreddit_posts += 1\n",
    "                                    total_posts_found += 1\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"      Error processing submission {submission.id}: {str(e)}\")\n",
    "                                    continue\n",
    "                        \n",
    "                        if strategy_posts > 0:\n",
    "                            print(f\"    {strategy_name}: {strategy_posts} posts\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error with {strategy_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"  📊 r/{subreddit_name} total: {subreddit_posts} posts\")\n",
    "                self.rate_limit()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error accessing r/{subreddit_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n✅ Comprehensive scraping completed!\")\n",
    "        print(f\"📊 Total posts collected: {total_posts_found:,}\")\n",
    "        print(f\"📅 Time span: {self.start_date} to {self.end_date}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def assess_discussion_quality(self, submission):\n",
    "        \"\"\"Assess the quality of discussion for discourse analysis\"\"\"\n",
    "        if submission.num_comments > 100 and submission.score > 500:\n",
    "            return 'exceptional'\n",
    "        elif submission.num_comments > 50 and submission.score > 200:\n",
    "            return 'very_high'\n",
    "        elif submission.num_comments > 20 and submission.score > 50:\n",
    "            return 'high'\n",
    "        elif submission.num_comments > 10 and submission.score > 20:\n",
    "            return 'medium'\n",
    "        elif submission.num_comments > 3 and submission.score > 5:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'minimal'\n",
    "    \n",
    "    def assess_consciousness_relevance(self, text):\n",
    "        \"\"\"Assess how relevant the post is to consciousness discussions\"\"\"\n",
    "        consciousness_terms = [\n",
    "            'conscious', 'consciousness', 'sentient', 'sentience', 'aware', 'awareness',\n",
    "            'mind', 'intelligence', 'feeling', 'emotion', 'thinking', 'understanding',\n",
    "            'soul', 'alive', 'experience', 'subjective', 'qualia', 'phenomenal',\n",
    "            'self-aware', 'cognition', 'cognitive', 'mental', 'thoughts', 'perception'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        term_count = sum(1 for term in consciousness_terms if term in text_lower)\n",
    "        \n",
    "        if term_count >= 8:\n",
    "            return 'exceptional'\n",
    "        elif term_count >= 5:\n",
    "            return 'very_high'\n",
    "        elif term_count >= 3:\n",
    "            return 'high'\n",
    "        elif term_count >= 2:\n",
    "            return 'medium'\n",
    "        elif term_count >= 1:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'minimal'\n",
    "    \n",
    "    def analyze_comments(self, comments):\n",
    "        \"\"\"Analyze comment patterns for discourse analysis\"\"\"\n",
    "        if not comments:\n",
    "            return json.dumps({'total': 0, 'avg_length': 0, 'engagement_pattern': 'none'})\n",
    "        \n",
    "        total_comments = len(comments)\n",
    "        avg_length = sum(len(c['body']) for c in comments) / total_comments\n",
    "        avg_score = sum(c['score'] for c in comments) / total_comments\n",
    "        \n",
    "        # Determine engagement pattern\n",
    "        if avg_score > 20:\n",
    "            engagement_pattern = 'very_high_engagement'\n",
    "        elif avg_score > 10:\n",
    "            engagement_pattern = 'high_engagement'\n",
    "        elif avg_score > 3:\n",
    "            engagement_pattern = 'medium_engagement'\n",
    "        elif avg_score > 0:\n",
    "            engagement_pattern = 'low_engagement'\n",
    "        else:\n",
    "            engagement_pattern = 'negative_engagement'\n",
    "        \n",
    "        analysis = {\n",
    "            'total_comments': total_comments,\n",
    "            'avg_comment_length': round(avg_length, 1),\n",
    "            'avg_comment_score': round(avg_score, 1),\n",
    "            'engagement_pattern': engagement_pattern,\n",
    "            'discourse_depth': 'deep' if total_comments >= 8 else 'shallow'\n",
    "        }\n",
    "        \n",
    "        return json.dumps(analysis)\n",
    "    \n",
    "    def save_comprehensive_data(self, data):\n",
    "        \"\"\"Save comprehensive dataset with temporal analysis\"\"\"\n",
    "        if not data:\n",
    "            print(f\"❌ No data collected\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n💾 Saving comprehensive dataset...\")\n",
    "        \n",
    "        # Create main dataset\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Sort by creation date for temporal analysis\n",
    "        df = df.sort_values('created_utc')\n",
    "        \n",
    "        # Add temporal analysis columns\n",
    "        df['created_datetime'] = pd.to_datetime(df['created_date'])\n",
    "        df['week_of_year'] = df['created_datetime'].dt.isocalendar().week\n",
    "        df['quarter'] = df['created_datetime'].dt.quarter\n",
    "        df['year_quarter'] = df['created_year'].astype(str) + '-Q' + df['quarter'].astype(str)\n",
    "        \n",
    "        # Save main comprehensive dataset\n",
    "        main_filename = \"reddit_ai_conscious_comprehensive_2022_2025.csv\"\n",
    "        main_filepath = os.path.join(self.output_dir, main_filename)\n",
    "        df.to_csv(main_filepath, index=False, encoding='utf-8')\n",
    "        print(f\"✅ Main dataset saved to: {main_filepath}\")\n",
    "        print(f\"📊 Total records: {len(df):,}\")\n",
    "        \n",
    "        # Save analysis-ready version (key columns only)\n",
    "        analysis_columns = [\n",
    "            'subreddit', 'title', 'content', 'author', 'created_date', 'created_year', \n",
    "            'created_month', 'year_quarter', 'days_since_lamda', 'score', 'num_comments',\n",
    "            'discussion_quality', 'consciousness_relevance', 'text_length', \n",
    "            'top_comments', 'comment_analysis', 'permalink'\n",
    "        ]\n",
    "        analysis_df = df[analysis_columns]\n",
    "        analysis_filename = \"reddit_ai_conscious_for_analysis.csv\"\n",
    "        analysis_filepath = os.path.join(self.output_dir, analysis_filename)\n",
    "        analysis_df.to_csv(analysis_filepath, index=False, encoding='utf-8')\n",
    "        print(f\"📊 Analysis dataset saved to: {analysis_filepath}\")\n",
    "        \n",
    "        # Save sample for quick inspection\n",
    "        sample_size = min(50, len(df))\n",
    "        sample_df = df.head(sample_size)\n",
    "        sample_filename = \"reddit_ai_conscious_sample.csv\"\n",
    "        sample_filepath = os.path.join(self.output_dir, sample_filename)\n",
    "        sample_df.to_csv(sample_filepath, index=False, encoding='utf-8')\n",
    "        print(f\"🔍 Sample dataset saved to: {sample_filepath}\")\n",
    "        \n",
    "        # Generate and save comprehensive statistics\n",
    "        self.generate_comprehensive_stats(df)\n",
    "    \n",
    "    def generate_comprehensive_stats(self, df):\n",
    "        \"\"\"Generate comprehensive temporal and content statistics\"\"\"\n",
    "        print(f\"\\n📊 Generating comprehensive statistics...\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_posts = len(df)\n",
    "        date_range = f\"{df['created_date'].min()} to {df['created_date'].max()}\"\n",
    "        unique_subreddits = df['subreddit'].nunique()\n",
    "        unique_authors = df['author'].nunique()\n",
    "        \n",
    "        # Temporal statistics\n",
    "        posts_by_year = df['created_year'].value_counts().sort_index()\n",
    "        posts_by_quarter = df['year_quarter'].value_counts().sort_index()\n",
    "        peak_discussion_month = df['created_month'].value_counts().idxmax()\n",
    "        \n",
    "        # Quality statistics\n",
    "        high_quality_posts = len(df[df['discussion_quality'].isin(['high', 'very_high', 'exceptional'])])\n",
    "        high_consciousness_posts = len(df[df['consciousness_relevance'].isin(['high', 'very_high', 'exceptional'])])\n",
    "        \n",
    "        # Engagement statistics\n",
    "        avg_score = df['score'].mean()\n",
    "        avg_comments = df['num_comments'].mean()\n",
    "        most_discussed_post = df.loc[df['num_comments'].idxmax()]\n",
    "        highest_scored_post = df.loc[df['score'].idxmax()]\n",
    "        \n",
    "        # Subreddit statistics\n",
    "        top_subreddits = df['subreddit'].value_counts().head(10)\n",
    "        \n",
    "        # Create comprehensive statistics\n",
    "        stats = {\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'total_posts': total_posts,\n",
    "            'date_range': date_range,\n",
    "            'time_span_days': (df['created_utc'].max() - df['created_utc'].min()) / 86400,\n",
    "            'unique_subreddits': unique_subreddits,\n",
    "            'unique_authors': unique_authors,\n",
    "            'peak_discussion_month': peak_discussion_month,\n",
    "            'high_quality_posts': high_quality_posts,\n",
    "            'high_consciousness_relevance': high_consciousness_posts,\n",
    "            'avg_score': round(avg_score, 2),\n",
    "            'avg_comments': round(avg_comments, 2),\n",
    "            'most_discussed_post_title': most_discussed_post['title'][:100],\n",
    "            'most_discussed_post_comments': most_discussed_post['num_comments'],\n",
    "            'highest_scored_post_title': highest_scored_post['title'][:100],\n",
    "            'highest_scored_post_score': highest_scored_post['score'],\n",
    "            'posts_2022': posts_by_year.get(2022, 0),\n",
    "            'posts_2023': posts_by_year.get(2023, 0),\n",
    "            'posts_2024': posts_by_year.get(2024, 0),\n",
    "            'posts_2025': posts_by_year.get(2025, 0),\n",
    "        }\n",
    "        \n",
    "        # Save comprehensive statistics\n",
    "        stats_df = pd.DataFrame([stats])\n",
    "        stats_filename = \"reddit_ai_conscious_comprehensive_statistics.csv\"\n",
    "        stats_filepath = os.path.join(self.output_dir, stats_filename)\n",
    "        stats_df.to_csv(stats_filepath, index=False)\n",
    "        print(f\"📊 Statistics saved to: {stats_filepath}\")\n",
    "        \n",
    "        # Save temporal breakdown\n",
    "        temporal_data = []\n",
    "        for year in posts_by_year.index:\n",
    "            temporal_data.append({\n",
    "                'year': year,\n",
    "                'total_posts': posts_by_year[year],\n",
    "                'avg_score': df[df['created_year'] == year]['score'].mean(),\n",
    "                'avg_comments': df[df['created_year'] == year]['num_comments'].mean(),\n",
    "                'high_quality_posts': len(df[(df['created_year'] == year) & \n",
    "                                           (df['discussion_quality'].isin(['high', 'very_high', 'exceptional']))])\n",
    "            })\n",
    "        \n",
    "        temporal_df = pd.DataFrame(temporal_data)\n",
    "        temporal_filename = \"reddit_ai_conscious_temporal_breakdown.csv\"\n",
    "        temporal_filepath = os.path.join(self.output_dir, temporal_filename)\n",
    "        temporal_df.to_csv(temporal_filepath, index=False)\n",
    "        print(f\"📅 Temporal breakdown saved to: {temporal_filepath}\")\n",
    "        \n",
    "        # Save subreddit breakdown\n",
    "        subreddit_stats = []\n",
    "        for subreddit in top_subreddits.head(15).index:\n",
    "            sub_df = df[df['subreddit'] == subreddit]\n",
    "            subreddit_stats.append({\n",
    "                'subreddit': subreddit,\n",
    "                'total_posts': len(sub_df),\n",
    "                'avg_score': sub_df['score'].mean(),\n",
    "                'avg_comments': sub_df['num_comments'].mean(),\n",
    "                'high_quality_posts': len(sub_df[sub_df['discussion_quality'].isin(['high', 'very_high', 'exceptional'])]),\n",
    "                'date_range': f\"{sub_df['created_date'].min()} to {sub_df['created_date'].max()}\"\n",
    "            })\n",
    "        \n",
    "        subreddit_df = pd.DataFrame(subreddit_stats)\n",
    "        subreddit_filename = \"reddit_ai_conscious_subreddit_breakdown.csv\"\n",
    "        subreddit_filepath = os.path.join(self.output_dir, subreddit_filename)\n",
    "        subreddit_df.to_csv(subreddit_filepath, index=False)\n",
    "        print(f\"📊 Subreddit breakdown saved to: {subreddit_filepath}\")\n",
    "        \n",
    "        # Display key statistics\n",
    "        print(f\"\\n📊 === COMPREHENSIVE AI CONSCIOUS DISCOURSE ANALYSIS ===\")\n",
    "        print(f\"📅 Period: {date_range}\")\n",
    "        print(f\"📊 Total Posts: {total_posts:,}\")\n",
    "        print(f\"📅 Time Span: {stats['time_span_days']:.0f} days\")\n",
    "        print(f\"🏛️  Coverage: {unique_subreddits} subreddits\")\n",
    "        print(f\"👥 Authors: {unique_authors:,}\")\n",
    "        print(f\"🏆 High Quality Posts: {high_quality_posts:,}\")\n",
    "        print(f\"🧠 High Consciousness Relevance: {high_consciousness_posts:,}\")\n",
    "        print(f\"📈 Average Score: {avg_score:.1f}\")\n",
    "        print(f\"💬 Average Comments: {avg_comments:.1f}\")\n",
    "        print(f\"📅 Peak Discussion Month: {peak_discussion_month}\")\n",
    "        \n",
    "        print(f\"\\n📊 Posts by Year:\")\n",
    "        for year, count in posts_by_year.items():\n",
    "            print(f\"   {year}: {count:,} posts\")\n",
    "        \n",
    "        print(f\"\\n🏛️  Top Subreddits:\")\n",
    "        for subreddit, count in top_subreddits.head(5).items():\n",
    "            print(f\"   r/{subreddit}: {count:,} posts\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def test_reddit_connection(self):\n",
    "        \"\"\"Test Reddit API connection\"\"\"\n",
    "        try:\n",
    "            test_subreddit = self.reddit.subreddit('artificial')\n",
    "            print(f\"✅ Reddit API: Connected - r/artificial has {test_subreddit.subscribers:,} subscribers\")\n",
    "            \n",
    "            # Test search functionality\n",
    "            test_search = list(test_subreddit.search(self.keyword, limit=3))\n",
    "            print(f\"✅ Search function: Working - found {len(test_search)} test results for '{self.keyword}'\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Reddit API connection failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def run_comprehensive_scraping(self):\n",
    "        \"\"\"Run the comprehensive AI conscious discourse scraping\"\"\"\n",
    "        print(\"🎯 === COMPREHENSIVE AI CONSCIOUS DISCOURSE SCRAPER ===\")\n",
    "        print(f\"📅 Analyzing 3+ years of 'AI conscious' discussions on Reddit\")\n",
    "        print(f\"📁 Output directory: {self.output_dir}\")\n",
    "        \n",
    "        # Test Reddit connection\n",
    "        if not self.test_reddit_connection():\n",
    "            print(\"❌ Cannot proceed without Reddit API access\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📊 Scraping Parameters:\")\n",
    "        print(f\"   🔍 Keyword: '{self.keyword}'\")\n",
    "        print(f\"   📅 Period: {self.start_date} to {self.end_date}\")\n",
    "        print(f\"   🏛️  Subreddits: {len(self.subreddits)} communities\")\n",
    "        print(f\"   ⏱️  Expected Duration: 2-3 hours\")\n",
    "        \n",
    "        # Run comprehensive scraping\n",
    "        print(f\"\\n🚀 Starting comprehensive scraping...\")\n",
    "        all_data = self.scrape_ai_conscious_discussions()\n",
    "        \n",
    "        # Save all data\n",
    "        self.save_comprehensive_data(all_data)\n",
    "        \n",
    "        print(f\"\\n🎉 === COMPREHENSIVE SCRAPING COMPLETE ===\")\n",
    "        print(f\"📊 Total 'AI conscious' posts collected: {len(all_data):,}\")\n",
    "        print(f\"📁 All files saved in: {self.output_dir}\")\n",
    "        print(f\"📅 Dataset spans: {self.start_date} to {self.end_date}\")\n",
    "        print(f\"\\n📋 Generated Files:\")\n",
    "        print(f\"   • reddit_ai_conscious_comprehensive_2022_2025.csv (main dataset)\")\n",
    "        print(f\"   • reddit_ai_conscious_for_analysis.csv (analysis-ready)\")\n",
    "        print(f\"   • reddit_ai_conscious_sample.csv (preview)\")\n",
    "        print(f\"   • reddit_ai_conscious_comprehensive_statistics.csv\")\n",
    "        print(f\"   • reddit_ai_conscious_temporal_breakdown.csv\")\n",
    "        print(f\"   • reddit_ai_conscious_subreddit_breakdown.csv\")\n",
    "\n",
    "# Script execution\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ComprehensiveAIConsciousScraper()\n",
    "    scraper.run_comprehensive_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
